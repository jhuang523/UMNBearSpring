{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9bc7abf",
   "metadata": {},
   "source": [
    "# Bear Spring EPM (Equivalent Porous Media) Model\n",
    "\n",
    "Model started 10/14/24. Modeled after Langevin, C. 2024, UMN Hydrocamp  \n",
    "[Link to Reference Code](https://github.com/langevin-usgs/umn2024/tree/main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb281706",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flopy'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 6\u001b[1;36m\n\u001b[1;33m    import flopy #Used for interacting with MODFLOW, developing MODFLOW inputs and reading outputs\u001b[1;36m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m\u001b[1;31m:\u001b[0m No module named 'flopy'\n"
     ]
    }
   ],
   "source": [
    "# import python packages\n",
    "import pathlib as pl #Pathlib used for switching between directories, accessing files\n",
    "import numpy as np #for numerical operations\n",
    "import matplotlib.pyplot as plt #for creating plots\n",
    "import pandas as pd #for handling dataframes\n",
    "import flopy #Used for interacting with MODFLOW, developing MODFLOW inputs and reading outputs\n",
    "import rasterio #For working with raster data and shapefiles\n",
    "import geopandas as gpd #For importing shapefiles\n",
    "import shapely as shp #For handling shapely geometries\n",
    "from shapely import vectorized # For building the idomain using vector arrays\n",
    "import networkx as nx #For handling creek networks and pathways\n",
    "import geojson #For handling geojson files\n",
    "import imageio.v2 as imageio #For making GIFs of head distributions\n",
    "from datetime import datetime, timedelta #For creating datetime timeseries for plotting\n",
    "import shutil as shutil #For saving and creating directories of head plots in transient\n",
    "import pygeos #For handling geographic data\n",
    "from pygeos import STRtree #For handling geographic data in stringtrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712c859d",
   "metadata": {},
   "source": [
    "## Load External Data\n",
    "\n",
    "Import the watershed data (HUC - 12)  \n",
    "Import the Sinkhole and Spring Data  \n",
    "Import the Bear Spring Discharge Data  \n",
    "Import the Groundwater Elevation Data from the MRSW  \n",
    "Import the Recharge Data from Zixuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa50d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Watershed Extent data from USGS StreamStats\n",
    "geoJSON = './data/HUC_12.geojson'\n",
    "watershed = gpd.read_file(geoJSON)\n",
    "projected_crs = 'EPSG:32615' #Set the projection coordinate ref. to the correct region\n",
    "watershed = watershed.to_crs(projected_crs) #Set the projected CRS to the watershed polygon\n",
    "\n",
    "#Check for geometry types (need to extract a polygon dataset)\n",
    "watershed_types = watershed.geometry.type.unique()\n",
    "print(f'The data types in watershed_types is: {watershed_types}')\n",
    "\n",
    "#Generate a Polygon object from the watershed geoJSON\n",
    "wsPolygon = watershed[watershed.geometry.type == 'Polygon']\n",
    "#repair the geometry for any errors that make the polygon not valid\n",
    "wsPolygon.loc[:,'geometry'] = wsPolygon['geometry'].apply(shp.validation.make_valid)\n",
    "print(f'The polygon is Valid: {wsPolygon.is_valid}')\n",
    "wsPolygon = wsPolygon.explode(index_parts=True)\n",
    "\n",
    "#get the bounding box for the watershed polygon\n",
    "bounds = wsPolygon.total_bounds\n",
    "print(f'min x: {bounds[0]}, min y: {bounds[1]}, max x: {bounds[2]}, max y: {bounds[3]}')\n",
    "\n",
    "#Get the area of the watershed polygon\n",
    "wsArea = wsPolygon['geometry'].area.sum()\n",
    "print(f'Watershed area: {wsArea/1e6:.2f} Km^2')\n",
    "\n",
    "#Import the karst feature data (UTME, UTMN, elevation data)\n",
    "f = pl.Path('./data/karst_features.csv')\n",
    "karst_df = pd.read_csv(f)\n",
    "karst_df = karst_df.set_index(\"ID\") #Set the index for the karst dataframe to the feature name\n",
    "karst_df['Elevation_m'] = karst_df['Elevation_ft'] * 0.3048 # create a new col. with the m elevation\n",
    "\n",
    "#Isolate the spring and sinkhole coordinates for plotting on DEM\n",
    "springs = karst_df[karst_df['Type'] == 'spring' ] #get springs\n",
    "sinkholes = karst_df[karst_df['Type'] == 'sink' ] #get sinkholes\n",
    "\n",
    "#Import the sinkhole coords that have NOT been traced\n",
    "untracedSinks = np.loadtxt('./data/bear_sinkholes_coords.txt')\n",
    "\n",
    "#Import the creek data shapefile\n",
    "creeks = gpd.read_file('./data/creeks_geoJSON.geojson')\n",
    "creeks = creeks.to_crs(wsPolygon.crs) #Set the coordinate reference system for the shapefile\n",
    "creeks = gpd.clip(creeks, wsPolygon) #Mask out all the shapefile data that is not inside of the wsPolygon\n",
    "\n",
    "#Import the Bear Spring discharge data\n",
    "f = pl.Path('./data/transient/2019_selected_discharge.csv')\n",
    "obsDis_df = pd.read_csv(f, parse_dates=[\"DATETIME\"], index_col=\"DATETIME\")\n",
    "# Convert the index to datetime, automatically handling mixed formats\n",
    "obsDis_df.index = pd.to_datetime(obsDis_df.index, format='mixed')\n",
    "\n",
    "#Get the Marion Rest Stop Well data \n",
    "f = pl.Path('./data/transient/2019_selected_MRSW_gwElev.csv')\n",
    "obsGWE_df = pd.read_csv(f, parse_dates=[\"datetime\"], index_col=\"datetime\")\n",
    "#Convert the index to datetime objects\n",
    "obsGWE_df.index = pd.to_datetime(obsGWE_df.index)\n",
    "#Get rid of any NaN rows produced by the data filtering outside of python\n",
    "obsGWE_df.dropna(inplace=True)\n",
    "obsGWE_df.replace(0, np.nan, inplace=True)\n",
    "\n",
    "#Get the recharge from Zixuan to try (no ET)\n",
    "f = pl.Path('./data/transient/2019_selected_recharge.csv')\n",
    "rch_df = pd.read_csv(f,parse_dates=['DATE'], index_col='DATE')\n",
    "rch_df.index = pd.to_datetime(rch_df.index) # Reset the index to be datetime objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be94671",
   "metadata": {},
   "source": [
    "Import the Springshed polygon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac254240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the springshed polygon data\n",
    "geoJSON = './data/Bear_Springshed.geojson'\n",
    "springshed = gpd.read_file(geoJSON)\n",
    "projected_crs = 'EPSG:32615'\n",
    "springshed = springshed.to_crs(projected_crs)\n",
    "\n",
    "#Check for geometry types (need to extract a polygon dataset)\n",
    "springshed_types = springshed.geometry.type.unique()\n",
    "print(f'The data tpyes in the springshed geoJSON: {springshed_types}')\n",
    "\n",
    "#Generate a Polygon object from the watershed geoJSON\n",
    "bsPolygon = springshed[springshed.geometry.type == 'Polygon']\n",
    "\n",
    "#repair the geometry for any errors that make the polygon not valid\n",
    "bsPolygon.loc[:,'geometry'] = bsPolygon['geometry'].apply(shp.validation.make_valid)\n",
    "print(f'The polygon is Valid: {bsPolygon.is_valid}')\n",
    "bsPolygon = bsPolygon.explode(index_parts=True)\n",
    "\n",
    "#Print the area of Bear Springshed\n",
    "bsArea = bsPolygon['geometry'].area.sum()\n",
    "print(f'Springshed Area: {bsArea/1e6:.2f} Km^2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9129e5",
   "metadata": {},
   "source": [
    "### Generate Helper Functions  \n",
    "To be used later in the code for various purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a9aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get karst feature data from karst_df\n",
    "def get_karst_data(ID):\n",
    "    return karst_df.loc[ID].to_dict()\n",
    "\n",
    "#Get DEM elevation data for a karst feature point\n",
    "def get_cell_elev(ID):\n",
    "    k_feat = get_karst_data(ID)\n",
    "    i, j = sg.intersect(k_feat['UTME'], k_feat['UTMN'])\n",
    "    elev = dem_grid[i,j]\n",
    "    #print(f'Feature ID: {ID},row: {i}, col: {j}, DEM_elev: {elev} ')\n",
    "    return elev, i, j\n",
    "\n",
    "#Establish the creek cells using a different method than that above, works better with coarse grids\n",
    "def get_creek_cells(creek, sg, dem_grid, idomain, conductance=50):\n",
    "    river_cells = set()\n",
    "\n",
    "    # Convert creek geometry to PyGEOS geometries\n",
    "    creek_geoms = [pygeos.linestrings(line.coords) for line in creek.geometry]\n",
    "    creek_tree = STRtree(creek_geoms)\n",
    "\n",
    "    # Iterate through grid cells\n",
    "    for i in range(sg.nrow):\n",
    "        for j in range(sg.ncol):\n",
    "            if idomain[0, i, j] == 1:  # Only check active cells\n",
    "                # Get cell vertices and create a PyGEOS Polygon\n",
    "                vertices = sg.get_cell_vertices(i, j)\n",
    "                cell_poly = pygeos.polygons(vertices)\n",
    "\n",
    "                # Query the spatial index for intersecting creek geometries\n",
    "                intersecting_indices = creek_tree.query(cell_poly, predicate=\"intersects\")\n",
    "\n",
    "                # If there are intersections, record the cell\n",
    "                if len(intersecting_indices) > 0:\n",
    "                    elev = dem_grid[i, j]\n",
    "                    river_cells.add((0, i, j, elev, conductance))\n",
    "\n",
    "    return list(river_cells)\n",
    "\n",
    "# Ensure \"island\" cells are deactivated using flood-fill algorithm\n",
    "def remove_isolated_cells(idomain):\n",
    "    for k in range(idomain.shape[0]):  # Iterate over layers\n",
    "        # Label connected components of active cells\n",
    "        labeled_array, num_features = scipy.ndimage.label(idomain[k] == 1)\n",
    "        \n",
    "        # Find sizes of each connected component\n",
    "        component_sizes = np.bincount(labeled_array.ravel())\n",
    "        \n",
    "        # Keep only the largest connected component (or set threshold for size)\n",
    "        largest_component = np.argmax(component_sizes[1:]) + 1  # Ignore 0 (inactive cells)\n",
    "        \n",
    "        # Deactivate isolated cells\n",
    "        idomain[k][labeled_array != largest_component] = 0\n",
    "\n",
    "#Get locations of cells in list form\n",
    "nrow = None #Just to get the error to stop showing up (is defined later)\n",
    "ncol = None #Just to get the error to stop showing up (is defined later)\n",
    "def get_nodes(locs):\n",
    "    nodes = []\n",
    "    for k, i, j in locs:\n",
    "        nodes.append(k * nrow * ncol + i * ncol + j)\n",
    "    print(f'The nodes are: {nodes}')\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da267de8",
   "metadata": {},
   "source": [
    "## Build the domain from the above GeoJSON, shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9906ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the polygon using the correct CRS to ensure that the bounds are in the right coordinate system and that the polygon looks correct\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "wsPolygon.plot(ax=ax, color='lightblue', edgecolor='black', alpha = 0.25, label = 'Watershed Area')\n",
    "bsPolygon.plot(ax=ax, color='lightcoral', edgecolor='black', alpha = 0.25, label = 'Springshed Area')\n",
    "creeks.plot(ax=ax, color='blue', label='Bear Creek')\n",
    "plt.xlabel('Easting (m)')\n",
    "plt.ylabel('Northing (m)')\n",
    "plt.title('Watershed Polygon Plot on UTM Grid')\n",
    "plt.grid(True)\n",
    "#Plot the springs and sinkholes on this grid\n",
    "plt.scatter(springs['UTME'], springs['UTMN'], color = 'green', label='Springs')\n",
    "plt.scatter(sinkholes['UTME'], sinkholes['UTMN'], color='orange', label = 'Sinkholes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a5788",
   "metadata": {},
   "source": [
    "### Combine the Watershed Polygon with the Springshed Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a6b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the polygons into a collective list\n",
    "polygons = list(wsPolygon['geometry']) + list(bsPolygon['geometry'])\n",
    "\n",
    "#Merge the polygon list into a singular polygon covering the watershed and springshed\n",
    "merged = shp.ops.unary_union(polygons)\n",
    "\n",
    "#Convert the merged polygon to a GeoDataFrame (gdf) object\n",
    "wsMerged = gpd.GeoDataFrame([{'geometry': merged}], crs=projected_crs)\n",
    "\n",
    "#repair the geometry for any errors that make the polygon not valid\n",
    "wsMerged.loc[:,'geometry'] = wsMerged['geometry'].apply(shp.validation.make_valid)\n",
    "print(f'The polygon is Valid: {wsMerged.is_valid}')\n",
    "wsMerged = wsMerged.explode(index_parts=True)\n",
    "\n",
    "#Plot the new domain shape from the merged polygons\n",
    "label = False #Flag for turning the creek linestring labels on/off\n",
    "#Plot the creek and karst feature data on the watershed\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "wsMerged.plot(ax=ax, color = 'lightblue', edgecolor = 'black')\n",
    "creeks.plot(ax=ax, color='blue', label='Bear Creek')\n",
    "\n",
    "#Label each of the linestrings for the creeks GDF\n",
    "if label == True:\n",
    "    for _, creek in creeks.iterrows():\n",
    "        creek_geom = creek['geometry']\n",
    "\n",
    "        midpoint = creek_geom.interpolate(0.5, normalized = True)\n",
    "\n",
    "        ax.text(midpoint.x, midpoint.y, str(creek['FID']), color='k', fontsize=12, ha='center')\n",
    "\n",
    "plt.xlabel('Easting (m)')\n",
    "plt.ylabel('Northing (m)')\n",
    "plt.title('Merged Watershed Polygon Plot on UTM Grid')\n",
    "plt.grid(True)\n",
    "#Plot the springs and sinkholes on this grid\n",
    "plt.scatter(springs['UTME'], springs['UTMN'], color = 'green', label='Springs')\n",
    "plt.scatter(sinkholes['UTME'], sinkholes['UTMN'], color='orange', label = 'Sinkholes')\n",
    "plt.scatter(552594.407, 4870544.353, color='red')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#Get the new bounds of the merged polygon\n",
    "bounds = wsMerged.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd78545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the stream segments to extend to the watershed boundary\n",
    "fids = [186, 154]\n",
    "select_creeks = creeks[creeks['FID'].isin(fids)]\n",
    "\n",
    "coords0 = list(select_creeks.iloc[0].geometry.coords)[-1]\n",
    "coords0 = shp.geometry.Point(coords0)\n",
    "coords1 = list(select_creeks.iloc[1].geometry.coords)[-1]\n",
    "coords1 = shp.geometry.Point(coords1)\n",
    "\n",
    "#Get the boundary for the watershed\n",
    "wsBoundary = wsMerged.geometry.iloc[0].boundary\n",
    "\n",
    "#Find the nearest points from the end of the selected creeks to the boundary\n",
    "nearest0 = shp.ops.nearest_points(coords0, wsBoundary)\n",
    "nearest1 = shp.ops.nearest_points(coords1, wsBoundary)\n",
    "nearest_point0 = nearest0[1]\n",
    "nearest_point1 = nearest1[1]\n",
    "\n",
    "#Plot the new points\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "wsMerged.boundary.plot(ax=ax, color = 'k', edgecolor = 'black')\n",
    "select_creeks.plot(ax=ax, color='blue', label='Bear Creek')\n",
    "plt.scatter(nearest_point0.x, nearest_point0.y, color = 'red')\n",
    "plt.scatter(coords0.x, coords0.y, color = 'green')\n",
    "plt.scatter(nearest_point1.x, nearest_point1.y, color = 'red')\n",
    "plt.scatter(coords1.x, coords1.y, color = 'green')\n",
    "\n",
    "#Now add in the line segment connecting the\n",
    "#creeks to the boundary cells nearest to them\n",
    "#build new line segments\n",
    "segment0 = shp.geometry.LineString([coords0, nearest_point0])\n",
    "segment1 = shp.geometry.LineString([coords1, nearest_point1])\n",
    "\n",
    "#convert line segments to geodataframe objects\n",
    "segment0_gdf = gpd.GeoDataFrame(\n",
    "    {'FID': [max(select_creeks['FID']) + 1]},  # Assign a unique FID\n",
    "    geometry=[segment0],\n",
    "    crs=select_creeks.crs  # Use the same CRS as the original creeks GeoDataFrame\n",
    ")\n",
    "\n",
    "segment1_gdf = gpd.GeoDataFrame(\n",
    "    {'FID': [max(select_creeks['FID']) + 1]},  # Assign a unique FID\n",
    "    geometry=[segment1],\n",
    "    crs=select_creeks.crs  # Use the same CRS as the original creeks GeoDataFrame\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468cf0c",
   "metadata": {},
   "source": [
    "### Connect the Existing Creek Boundary to the edge of the Watershed Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa48fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the creeks together with the selected creeks\n",
    "new_creeks = pd.concat([select_creeks, segment0_gdf], ignore_index=True)\n",
    "new_creeks = pd.concat([new_creeks, segment1_gdf], ignore_index=True)\n",
    "\n",
    "#combine the newly made creeks with the existing creek deliniations\n",
    "creeks = pd.concat([creeks, new_creeks], ignore_index=True)\n",
    "\n",
    "#Find the intersection points where the boundary intersects with the creek\n",
    "intersections = creeks.geometry.apply(lambda line: line.intersection(wsBoundary))\n",
    "intersections_points = intersections[intersections.geom_type == \"Point\"]\n",
    "\n",
    "unique_points = list({(point.x, point.y) for point in intersections_points})\n",
    "\n",
    "#Plot the new creeks and the intersection points on a plot\n",
    "label = False #Flag for turning the creek linestring labels on/off\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "wsMerged.boundary.plot(ax=ax, color = 'k', edgecolor = 'black')\n",
    "creeks.plot(ax=ax, color='blue', label='Bear Creek')\n",
    "for i in range(len(unique_points)):\n",
    "    plt.scatter(unique_points[i][0], unique_points[i][1], color='red')\n",
    "\n",
    "if label == True:    \n",
    "    for _, creek in creeks.iterrows():\n",
    "        creek_geom = creek['geometry']\n",
    "\n",
    "        midpoint = creek_geom.interpolate(0.5, normalized = True)\n",
    "\n",
    "        ax.text(midpoint.x, midpoint.y, str(creek['FID']), color='k', fontsize=12, ha='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe075c54",
   "metadata": {},
   "source": [
    "### Isolate the area between the Three points as the new Subdomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the selected points above to generate a new domain\n",
    "print(unique_points)\n",
    "\n",
    "G = nx.Graph()\n",
    "#Define a function to find the shortest path from each point through the creek network\n",
    "for idx, line in creeks.iterrows():\n",
    "    coords = list(line.geometry.coords)\n",
    "    G.add_edge(tuple(coords[0]), tuple(coords[-1]), geometry = line.geometry)\n",
    "    \n",
    "#build a fxn for finding the shortest path between two points in a network\n",
    "def find_shortest_path(G, start_point, end_point):\n",
    "    #Find the nearest node for the start and end points\n",
    "    start_node = min(G.nodes, key=lambda node: shp.geometry.Point(node).distance(start_point))\n",
    "    end_node = min(G.nodes, key=lambda node: shp.geometry.Point(node).distance(end_point))\n",
    "    \n",
    "    #find the shortest path between these nodes\n",
    "    path = nx.shortest_path(G, source=start_node, target=end_node)\n",
    "    return path\n",
    "\n",
    "#Convert the coordinate tuples to point objects\n",
    "points = [shp.geometry.Point(coords) for coords in unique_points]\n",
    "\n",
    "#find shortest path along creeks\n",
    "pathAC = find_shortest_path(G, points[0], points[2])\n",
    "pathBC = find_shortest_path(G, points[1], points[2])\n",
    "\n",
    "#Convert these paths (sets of nodes) into LineStrings\n",
    "linesAC = [G.edges[pathAC[i], pathAC[i + 1]]['geometry'] for i in range(len(pathAC) - 1)]\n",
    "linesBC = [G.edges[pathBC[i], pathBC[i + 1]]['geometry'] for i in range(len(pathBC) - 1)]\n",
    "\n",
    "#Merge these lines into continuous paths\n",
    "AC = shp.ops.linemerge(linesAC)\n",
    "BC = shp.ops.linemerge(linesBC)\n",
    "\n",
    "#Get the domain boundary along the watershed boundary\n",
    "# Convert LineString to a list of coordinates\n",
    "boundary_coords = list(wsBoundary.coords)\n",
    "\n",
    "# Find nearest boundary coordinates for Points A and B\n",
    "def find_nearest_index(coords, point):\n",
    "    return np.argmin([shp.ops.Point(c).distance(point) for c in coords])\n",
    "\n",
    "index_a = find_nearest_index(boundary_coords, points[0])\n",
    "index_b = find_nearest_index(boundary_coords, points[1])\n",
    "\n",
    "# Create two paths: clockwise and counterclockwise\n",
    "if index_a < index_b:\n",
    "    path1 = boundary_coords[index_a:index_b + 1]\n",
    "    path2 = boundary_coords[index_b:] + boundary_coords[:index_a + 1]\n",
    "else:\n",
    "    path1 = boundary_coords[index_b:index_a + 1]\n",
    "    path2 = boundary_coords[index_a:] + boundary_coords[:index_b + 1]\n",
    "\n",
    "# Convert paths to LineStrings and select the shorter one\n",
    "path1_ls = shp.geometry.LineString(path1)\n",
    "path2_ls = shp.geometry.LineString(path2)\n",
    "\n",
    "wsSegment = path1_ls if path1_ls.length < path2_ls.length else path2_ls\n",
    "\n",
    "#Get the boundary for the creek paths developed above\n",
    "#Get the projected points on the constructed lines\n",
    "\n",
    "#AC\n",
    "AC_start = AC.project(points[0])\n",
    "AC_end = AC.project(points[2])\n",
    "acSegment = shp.ops.substring(AC, AC_start, AC_end)\n",
    "\n",
    "#BC\n",
    "BC_start = BC.project(points[1])\n",
    "BC_end = BC.project(points[2])\n",
    "bcSegment = shp.ops.substring(BC, BC_start, BC_end)\n",
    "\n",
    "#Convert to a singular shape\n",
    "subDomain_coords = (\n",
    "    list(acSegment.coords) +\n",
    "    list(bcSegment.coords[::-1]) +\n",
    "    list(wsSegment.coords[::-1])\n",
    ")\n",
    "\n",
    "#create trianlge Polygon\n",
    "subDomain_poly = shp.geometry.Polygon(subDomain_coords)\n",
    "\n",
    "#Convert to GDF object\n",
    "# Plot the result\n",
    "subDomain = gpd.GeoDataFrame(\n",
    "    {'geometry': [subDomain_poly]},\n",
    "    crs=wsMerged.crs  # Use CRS of your original data\n",
    ")\n",
    "\n",
    "subArea = subDomain['geometry'].area.sum()/1e6\n",
    "print(f'Subdomain Area: {subArea:.2f} Km^2')\n",
    "\n",
    "#subDomain is the new domain that is selected between the two creeks\n",
    "# Plot\n",
    "ax = subDomain.plot(color=\"cyan\", edgecolor=\"black\", figsize=(10, 8))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39896d35",
   "metadata": {},
   "source": [
    "## Apply the subdomain above to the DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd48f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the DEM data\n",
    "with rasterio.open('./data/DEM_1m/dem_1m_m.bil') as src:\n",
    "    # Define the window of useful data (in UTM coords) from the larger DEM shapefile\n",
    "    window = rasterio.windows.from_bounds(bounds[0], bounds[1], bounds[2], bounds[3], transform=src.transform)\n",
    "    extent = rasterio.windows.bounds(window, src.transform)  # Get window boundaries to confirm they are correct\n",
    "    \n",
    "    # Read only the windowed portion of the DEM\n",
    "    dem_data = src.read(1, window=window)\n",
    "    \n",
    "    # Mask out the erroneous data (excessively large values due to data errors)\n",
    "    maxval = 10000\n",
    "    dem_data = np.ma.masked_where(dem_data > maxval, dem_data)\n",
    "    \n",
    "    # Get the corresponding transform for plotting\n",
    "    window_transform = src.window_transform(window)\n",
    "    \n",
    "#Plot DEM as a color plot to evaluate all of the values and establish the value range and units\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "dem_plot = ax.imshow(\n",
    "    dem_data,\n",
    "    cmap='terrain',\n",
    "    extent=[extent[0], extent[2], extent[1], extent[3]],\n",
    "    origin='upper'\n",
    ")\n",
    "# Add a colorbar for the DEM\n",
    "cbar = plt.colorbar(dem_plot, ax=ax, label='Elevation (m)')\n",
    "# Plot springs and sinkholes on the DEM\n",
    "ax.scatter(\n",
    "    springs['UTME'], springs['UTMN'],\n",
    "    color='green', label='BSS Springs', marker='o', edgecolor='black'\n",
    ")\n",
    "ax.scatter(\n",
    "    sinkholes['UTME'], sinkholes['UTMN'],\n",
    "    color='orange', label=' Traced Sinkholes', marker='x'\n",
    ")\n",
    "# Overlay the merged watershed (wsMerged GeoDataFrame)\n",
    "wsMerged.boundary.plot(ax=ax, color='k', linewidth=2, label='HUC-12 Watershed Boundary')\n",
    "#Overlay the Springshed Area (Should be within the merged area)\n",
    "bsPolygon.boundary.plot(ax=ax, color ='red', linewidth=2, label ='BSS Boundary')\n",
    "#Overlay the creeks\n",
    "creeks.plot(ax=ax, color= 'blue', linewidth=2, label='Creeks')\n",
    "#overlay the subdomain area between the creeks\n",
    "subDomain.boundary.plot(ax=ax, color= 'magenta', label= 'Subdomain Boundary', linewidth = 2)\n",
    "# Add labels and title\n",
    "ax.set_xlabel('UTME')\n",
    "ax.set_ylabel('UTMN')\n",
    "ax.set_title('Bear Springshed and Bear Creek Watershed', fontsize = 18)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#print the boundary values of the DEM\n",
    "for i in range(len(extent)):\n",
    "    print(f'Current UTM Boundary: {extent[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61214812",
   "metadata": {},
   "source": [
    "## Create the MODFLOW Model Grid\n",
    "\n",
    "Place the grid onto real world coordinates for use with the DEM data and geological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762df1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct a new set of bounds using the newly formed polygon\n",
    "bounds = subDomain.total_bounds\n",
    "print(f'min x: {bounds[0]}, min y: {bounds[1]}, max x: {bounds[2]}, max y: {bounds[3]}')\n",
    "\n",
    "#Re-adjust the cell size, domain of the model, ncol, nrow, based on the domain change\n",
    "Lx = bounds[2] - bounds[0]\n",
    "Ly = bounds[3] - bounds[1]\n",
    "#delr = 50.0 #m\n",
    "#delc = 50.0 #m\n",
    "delr = 100.0 #m\n",
    "delc = 100.0 #m\n",
    "ncol = int(Lx / delc)\n",
    "nrow = int(Ly / delr)\n",
    "\n",
    "#Re-import the new DEM_Grid with the updated boundaries from the subDomain\n",
    "with rasterio.open('./data/DEM_1m/dem_1m_m.bil') as src:\n",
    "    #define the window of useful data(in UTM coords) from the larger DEM shapefile\n",
    "    window = rasterio.windows.from_bounds(bounds[0], bounds[1], bounds[2], bounds[3], transform=src.transform)\n",
    "    extent = rasterio.windows.bounds(window, src.transform)#get window boundaries to confirm they are correct\n",
    "    \n",
    "    #convert all of the data to a grid so it can be used with the MODFLOW model\n",
    "    width = ncol#set the width of the grid\n",
    "    height = nrow #set the height of the grid\n",
    "    transform = rasterio.transform.from_bounds(*extent, width, height) #transform the data to the grid\n",
    "    \n",
    "    #Import the DEM and apply it to the grid\n",
    "    dem_grid = src.read(\n",
    "    1,\n",
    "    out_shape = (height, width),\n",
    "    window = window,\n",
    "    resampling= rasterio.enums.Resampling.bilinear\n",
    "    )\n",
    "    \n",
    "    #mask out the erroneous data (excessively large values due to data errors)\n",
    "    maxval = 10000\n",
    "    dem_grid = np.ma.masked_where(dem_grid > maxval, dem_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d5f7f",
   "metadata": {},
   "source": [
    "### Build Out the Modflow structured Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c085b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the grid properties top, bottom, nlay, that are not defined in the grid construction above\n",
    "nlay = 1\n",
    "top = dem_grid #Set the upper elevation boundary for the single layer model\n",
    "botm = np.ones((nlay, nrow, ncol), dtype = float) #contruct the array which will beomce the bottom array\n",
    "botm_elev = 325.044 #Top of the decorah shale THIS VALUE IS NOT CORRECT, ONLY IN TEMPORARILY\n",
    "#assign elevations for bottoms of the units\n",
    "botm[0] = botm_elev #This will eventually be changed by the angled botm array\n",
    "\n",
    "#Convert delc and delr to an array for input into the sg object\n",
    "delr = delr * np.ones(ncol, dtype = float)\n",
    "delc = delc * np.ones(nrow, dtype = float)\n",
    "\n",
    "# Generate the model grid in flopy\n",
    "sg = flopy.discretization.StructuredGrid(\n",
    "    nlay=nlay,\n",
    "    nrow=nrow,\n",
    "    ncol=ncol,\n",
    "    delr=delr,\n",
    "    delc=delc,\n",
    "    top=top,\n",
    "    botm=botm,\n",
    "    xoff=bounds[0],\n",
    "    yoff=bounds[1],\n",
    "    crs= 'EPSG:32615'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1883fd5",
   "metadata": {},
   "source": [
    "### Get the Coordinates for the sinkholes and the springs and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all of the coordinates for the sinkholes\n",
    "sinkhole_cells = [] #create an empty list for the sinkhole data\n",
    "for i in range(len(karst_df[karst_df['Type'] == 'sink'])):\n",
    "    df = karst_df[karst_df['Type'] == 'sink']\n",
    "    sinkhole = df.iloc[i]\n",
    "    elev, row, col = get_cell_elev(sinkhole.name)\n",
    "    sinkhole_cells.append((sinkhole.name, elev, row, col))\n",
    "\n",
    "for j in range(len(untracedSinks)):\n",
    "    x = untracedSinks[j][0]\n",
    "    y = untracedSinks[j][1]\n",
    "    i, j = sg.intersect(x, y)\n",
    "    sinkhole_cells.append(('Untraced', 0.00, i, j))\n",
    "\n",
    "#get all of the coordinates for the springs (NOT including Bear Spring)\n",
    "spring_cells = [] #create an empty list for the sinkhole data\n",
    "for i in range(len(karst_df[karst_df['Type'] == 'spring'])):\n",
    "    df = karst_df[karst_df['Type'] == 'spring']\n",
    "    spring = df.iloc[i]\n",
    "    elev, row, col = get_cell_elev(spring.name)\n",
    "    spring_cells.append((spring.name, elev, row, col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85d3673",
   "metadata": {},
   "source": [
    "### Build the idomain array using vector operations  \n",
    "Generally, this tends to be one of the slower steps in the model, especially at high resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idomain method based on vectorization and numpy ## This appears to be the fastsest of all the methods\n",
    "xc, yc = sg.xcellcenters, sg.ycellcenters #Get the cell center coords\n",
    "points = np.array([(xc[i, j], yc[i, j]) for i in range(nrow) for j in range(ncol)])\n",
    "\n",
    "#Extract the raw polygon from the GDF object subDomain\n",
    "domain = subDomain.geometry.values[0]\n",
    "domain = domain.buffer(0)\n",
    "gi = flopy.utils.GridIntersect(sg)\n",
    "idomain = np.zeros((nlay, nrow, ncol), dtype=int)\n",
    "mask = vectorized.contains(domain, points[:, 0], points[:, 1])\n",
    "idomain_mask = mask.reshape((nrow, ncol))\n",
    "idomain[:, :, :] = idomain_mask.astype(int)\n",
    "\n",
    "#Plot the idomain array for each layer of the model    \n",
    "fig, ax = plt.subplots(figsize=(9,7))  # Create subplots in a single row\n",
    "pmv = flopy.plot.PlotMapView(modelgrid=sg, layer=1)\n",
    "cb = pmv.plot_array(idomain[0], ax=ax)\n",
    "fig.colorbar(cb, ax=ax, orientation='horizontal', fraction=0.02, pad=0.1, shrink=0.8)\n",
    "plt.title('iDomain Array')\n",
    "plt.xlabel('UTME')\n",
    "plt.ylabel(\"UTMN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750254ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the DEM to the subDomain area and use it to generate a bottom array\n",
    "\n",
    "#Calculate the slope from the highest point in the active zone to the loweset point in the active zone\n",
    "# Mask the DEM for the active zone\n",
    "activeDEM = np.ma.masked_where(idomain[0] == 0, dem_grid)\n",
    "\n",
    "# Find max and min elevations within the active zone\n",
    "activeMax = np.max(activeDEM)\n",
    "activeMin = np.min(activeDEM)\n",
    "\n",
    "# Get the indices of max and min elevations in the masked array\n",
    "maxIndex = np.unravel_index(np.argmax(activeDEM), activeDEM.shape)\n",
    "minIndex = np.unravel_index(np.argmin(activeDEM), activeDEM.shape)\n",
    "\n",
    "# Plot the DEM with the max and min points\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 6))\n",
    "im = ax.imshow(activeDEM, cmap='terrain', origin='upper')\n",
    "plt.colorbar(im, ax=ax, label='Elevation (m)')\n",
    "ax.scatter(maxIndex[1], maxIndex[0], color='red', label='Max Elevation', zorder=10)\n",
    "ax.scatter(minIndex[1], minIndex[0], color='magenta', label='Min Elevation', zorder=10)\n",
    "ax.legend()\n",
    "ax.set_title('Active Zone with Elevation Extremes')\n",
    "plt.show()\n",
    "\n",
    "#calculate the distance between these two points in m\n",
    "#Get the UTME and UTMN coordinates for each of the cells\n",
    "UTME1 = bounds[0] + (maxIndex[1] + 0.5) * np.average(delc)\n",
    "UTME2 = bounds[0] + (minIndex[1] + 0.5) * np.average(delc)\n",
    "\n",
    "UTMN1 = bounds[1] + (100 - maxIndex[0] + 0.5) * np.average(delr) # 100 - because the plot here counts down on the y axis\n",
    "UTMN2 = bounds[1] + (100 - minIndex[0] + 0.5) * np.average(delr) # 100 - becaues the  plot here counts down on the y axis\n",
    "\n",
    "#Calculate the distance between each point, the elevation difference, and the slope\n",
    "wsH = activeMax - activeMin\n",
    "wsL = np.sqrt((UTME1-UTME2)**2 + (UTMN1-UTMN2)**2)\n",
    "wsSlope = wsH/wsL\n",
    "print(f'The overall surface slope of the watershed is {(wsSlope* 100):.2f} % to the NW')\n",
    "slopeDeg = np.arctan(wsSlope) * 180/np.pi\n",
    "print(f'The overall slope of the watershed in degrees is {slopeDeg:.2f} degrees')\n",
    "\n",
    "#determine the azimuth of the slope (to the NW)\n",
    "az = np.arctan((UTME1-UTME2)/(UTMN1-UTMN2))\n",
    "azDeg = az * (180/np.pi) + 360 #adding 360 because we want in azimuth convention\n",
    "print(f'The azimuth of the slope (max elev to min elev) is {azDeg:.2f} degrees')\n",
    "\n",
    "#The next step is to actually apply this azimuth and the known slope to create a bottom surface that slopes at ~1 degree in the same direction as the surface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf8a061",
   "metadata": {},
   "source": [
    "### Build Out the Bottom Array using the bottom surface slope of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc04da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert slope and azimuth to radians\n",
    "slopeRad = np.radians(slopeDeg)\n",
    "azRad = np.radians(azDeg)\n",
    "\n",
    "#Set the max elevation that is desired\n",
    "offset = 352.044\n",
    "\n",
    "# Compute slope components in physical coordinates\n",
    "slope_x = np.tan(slopeRad) * -np.sin(azRad)  # East-West component\n",
    "slope_y = np.tan(slopeRad) * np.cos(azRad)  # North-South component\n",
    "\n",
    "# Calculate physical grid dimensions\n",
    "total_width = ncol * np.average(delr)  # Total width of the grid in meters\n",
    "total_height = nrow * np.average(delc)  # Total height of the grid in meters\n",
    "\n",
    "# Scale slope contributions to grid coordinates\n",
    "slope_x_grid = slope_x * total_width / ncol\n",
    "slope_y_grid = slope_y * total_height / nrow\n",
    "\n",
    "# Construct the sloped surface\n",
    "col_indices = np.arange(ncol)  # Columns\n",
    "row_indices = np.arange(nrow)  # Rows\n",
    "\n",
    "# Bottom Sloped array with adjusted scaling\n",
    "botSloped = (offset\n",
    "            - slope_x_grid * col_indices[None, :]  # x-direction slope\n",
    "            - slope_y_grid * row_indices[:, None])  # y-direction slope\n",
    "\n",
    "#Rotate the matrix into the correct orientation (there is probably a math error above but I cannot find it, so this is my solution)\n",
    "botSloped = np.rot90(botSloped, 2)\n",
    "\n",
    "print(f'Max elev of the Sloped bottom array {np.max(botSloped)}')\n",
    "print(f'Min elev of the sloped bottom array {np.min(botSloped)}')\n",
    "\n",
    "#Plot the new sloped bottom\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "#Plot the array as a colorplot\n",
    "plot = ax.imshow(botSloped)\n",
    "\n",
    "#Add a colorbar\n",
    "plt.colorbar(plot)\n",
    "plt.title(f'Sloped Bottom Array, starting at elevation {offset}')\n",
    "\n",
    "#Use the botSloped array to replace the elvation set as the bottom of the model (botm[-1])\n",
    "botm[0] = botSloped\n",
    "\n",
    "#Use the new bottom array, as well assign new bounds to the model grid\n",
    "sg.botm[0] = botm[0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a 2D bresenham algorithm for identifying cell pathways.\n",
    "def bres(row1, col1, row2, col2):\n",
    "    \"\"\"Generate cell indices along a straight line in a 2D grid using Bresenham's algorithm.\"\"\"\n",
    "    cells = []\n",
    "    d_row = abs(row2 - row1)\n",
    "    d_col = abs(col2 - col1)\n",
    "    sign_row = 1 if row2 > row1 else -1\n",
    "    sign_col = 1 if col2 > col1 else -1\n",
    "    err = d_col - d_row\n",
    "\n",
    "    while (row1 != row2 or col1 != col2):\n",
    "        cells.append((row1, col1))\n",
    "        err2 = 2 * err\n",
    "        if err2 > -d_row:\n",
    "            err -= d_row\n",
    "            col1 += sign_col\n",
    "        if err2 < d_col:\n",
    "            err += d_col\n",
    "            row1 += sign_row\n",
    "\n",
    "    cells.append((row2, col2))  # Add the last cell\n",
    "    return cells\n",
    "\n",
    "#Test this out\n",
    "BSelev , iBS, jBS = get_cell_elev('55A00406') #get the elevation, cell location for Bear Spring\n",
    "#Get the elevation, cell location for 55D0000054\n",
    "testelev, itest, jtest = get_cell_elev('55D54')\n",
    "\n",
    "test_path = bres(iBS, jBS, itest, jtest)\n",
    "\n",
    "#This method appears to work as intended, so now let's apply it to each of the sinkholes\n",
    "conduit_cells = []\n",
    "for p in range(len(sinkhole_cells)):\n",
    "    row2 = sinkhole_cells[p][2]\n",
    "    col2 = sinkhole_cells[p][3]\n",
    "    lineCoords = bres(iBS, jBS, row2, col2)\n",
    "    conduit_cells.append(lineCoords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a803d72",
   "metadata": {},
   "source": [
    "## Assign Aquifer Properties (Kcond, Kh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Assign hydraulic conductivity (K) values for each of the layers\n",
    "Kh = 2.2 #m/day for the watershed outside of the springshed\n",
    "Kv = 1.5 #20.00 #m/day for the watershed outside of the springshed\n",
    "Kh = np.array(Kh)\n",
    "Kv = np.array(Kv)\n",
    "KhBPP = Kh #m/day for the springshed polygon area\n",
    "KvBPP = Kh # m/day for the springshed polygon area\n",
    "K11 = Kh * np.ones((nlay, nrow, ncol))\n",
    "K33 = Kv * np.ones((nlay, nrow, ncol))\n",
    "\n",
    "#Assign increased Kh and Kv values to the springshed area\n",
    "#extract the raw polygon from the GDF object bsPolygon\n",
    "bsdomain = bsPolygon.geometry.values[0]\n",
    "\n",
    "# This also appears to be a bit of  a slow step, needs to be addressed for the high resolution model\n",
    "\n",
    "ixbs = gi.intersect(bsdomain)\n",
    "for i, j in ixbs[\"cellids\"]:\n",
    "    K11[0, i, j] = KhBPP\n",
    "\n",
    "for i, j in ixbs[\"cellids\"]:\n",
    "    K33[0, i, j] = KvBPP\n",
    "\n",
    "Kcond = 10000 #m/day\n",
    "\n",
    "#THE TEXT BELOW IS WHAT IS USE TO ASSIGN THE CONDUIT CONDUCTIVITIES TO THE INDIVIDUAL CELLS\n",
    "#Adjust the conductivity values for the regions with the conduits\n",
    "for j in range(len(conduit_cells)):\n",
    "    path = conduit_cells[j]\n",
    "    #K_cond = conduit_conductivities[j]\n",
    "    K_cond = Kcond\n",
    "    for i in range(len(path)):\n",
    "        row = path[i][0]\n",
    "        col = path[i][1]\n",
    "        K11[0, row, col] = K_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae6ceb3",
   "metadata": {},
   "source": [
    "## Build and Run the Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c47233",
   "metadata": {},
   "source": [
    "Plot the Recharge Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97da2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (9,5))\n",
    "plt.plot(rch_df.index, rch_df['RCH[m/day]'])\n",
    "ax2 = ax.twinx()\n",
    "plt.plot(obsDis_df.index, obsDis_df['Discharge[m3/s]'], color ='orange')\n",
    "plt.title('Recharge Timeseries (hourly)', fontsize = 18)\n",
    "plt.xlabel('Date', fontsize = 14)\n",
    "plt.ylabel('Avg. Daily Precipitation Rate [m/day]', fontsize = 14)\n",
    "plt.xticks(fontsize = 10)\n",
    "plt.yticks(fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71faedd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Set up the model folder where outputs will be stored\n",
    "#ws = './EPM_wsgrid_transient_5m'\n",
    "ws = './EPM_transient_2019'\n",
    "name = 'EPM_transient'\n",
    "\n",
    "#generate the simulation object\n",
    "sim = flopy.mf6.MFSimulation(\n",
    "    sim_name=name,\n",
    "    sim_ws=ws,\n",
    "    exe_name='mf6',\n",
    "    version = 'mf6'\n",
    ")\n",
    "\n",
    "#Build the time inputs\n",
    "tuni = 'DAYS'\n",
    "#nper = len(precip_avg)\n",
    "nper = len(rch_df)\n",
    "nstp = 1 #Number of timesteps in each stress period\n",
    "perlen = 1 / 24 #length of each stress period\n",
    "#perlen = 1.0 # length of each stress period\n",
    "tsmult = 1.0 #Length of successive timesteps\n",
    "\n",
    "#Generate the tdis object (starting w/ tdis set up for steady state model)\n",
    "tdis = flopy.mf6.ModflowTdis(\n",
    "    sim,\n",
    "    pname = \"tdis\",\n",
    "    time_units = tuni,\n",
    "    nper = nper,\n",
    "    perioddata = [(perlen, nstp, tsmult) for _ in range(nper)]\n",
    ")\n",
    "\n",
    "#Create the IMS package (iterative model solver)\n",
    "ims = flopy.mf6.ModflowIms(\n",
    "    sim,\n",
    "    pname='ims',\n",
    "    inner_dvclose = 0.0001,\n",
    "    outer_dvclose = 0.0001,\n",
    "    linear_acceleration=\"BICGSTAB\",\n",
    "    complexity = \"SIMPLE\",\n",
    "    print_option = \"ALL\",\n",
    "    inner_maximum = 500,\n",
    "    outer_maximum = 50\n",
    "    #inner_maximum = 100,\n",
    "    #outer_maximum = 10\n",
    ")\n",
    "\n",
    "#Create the groundwater flow model\n",
    "gwf = flopy.mf6.ModflowGwf(\n",
    "    sim,\n",
    "    modelname = name,\n",
    "    save_flows = True,\n",
    "    newtonoptions = 'NEWTON UNDER_RELAXATION'\n",
    ")\n",
    "\n",
    "#create the discretization package\n",
    "dis = flopy.mf6.ModflowGwfdis(\n",
    "    gwf,\n",
    "    nlay = nlay,\n",
    "    nrow = nrow,\n",
    "    ncol = ncol,\n",
    "    delr = delr,\n",
    "    delc = delc,\n",
    "    top = top,\n",
    "    botm = botm,\n",
    "    xorigin = extent[0],\n",
    "    yorigin = extent[1],\n",
    "    idomain = idomain,\n",
    "    length_units = 'METERS'\n",
    ")\n",
    "\n",
    "# Set up the Recharge Package\n",
    "rch_spd = {i: rch_df['RCH[m/day]'][i] for i in range(nper)}\n",
    "rch = flopy.mf6.ModflowGwfrcha(\n",
    "    gwf,\n",
    "    print_input = True,\n",
    "    recharge = rch_spd,\n",
    "    pname = 'RCH',\n",
    "    save_flows = True\n",
    ")\n",
    "\n",
    "# #Set up the drain which will represent Bear Spring and the river\n",
    "BSelev , iBS, jBS = get_cell_elev('55A00406') #get the elevation, cell location for Bear Spring\n",
    "kBS = 0\n",
    "BS_conductance = Kcond * np.average(delr)\n",
    "BS_drain = [(kBS, iBS, jBS, BSelev, BS_conductance)] #build the Bear Spring drain list\n",
    "river_drain = get_creek_cells(creeks, sg, top, idomain, conductance= np.average(Kh) * np.average(delr)) #get the list of all the drain cell loations, elevs, conductances\n",
    "\n",
    "\n",
    "###########################################################\n",
    "##THIS IS USED TO ADDRESS AREAS WHERE THERE IS MISMATCH BETWEEN THE ACTIVE DOMAIN AND THE RIVER LOCATIONS\n",
    "#Get a list of all of the cells touching the border of the model not in the river_drain list\n",
    "#Pad the idomain matrix so that it works with the edge cases along the matrix boundaries\n",
    "idomain_pad = np.pad(idomain[0], pad_width=1, mode= 'constant', constant_values=0)\n",
    "\n",
    "#Get the cells along the border in a matrix\n",
    "adjacent_zeros = (\n",
    "    (idomain_pad[:-2, 1:-1] == 0) | #top border\n",
    "    (idomain_pad[2:, 1:-1] == 0) | #bottom border\n",
    "    (idomain_pad[1:-1, :-2] == 0) | #left border\n",
    "    (idomain_pad[1:-1, 2:] == 0) #right border\n",
    ")\n",
    "\n",
    "border_cells = np.argwhere(idomain == 1 & adjacent_zeros)\n",
    "\n",
    "#Filter out all the cells where there are already drains\n",
    "existing_drain_cells = {(k, i, j) for k, i, j, *_ in river_drain}\n",
    "\n",
    "unaddressed_cells = [tuple(cell) for cell in border_cells if tuple(cell) not in existing_drain_cells]\n",
    "new_cells = [cell for cell in unaddressed_cells if cell[2] <= 1000]\n",
    "\n",
    "#Create new entries for adding to the drain list\n",
    "new_river_drain_entries = [\n",
    "    (k, i, j, top[i, j], 1.0)\n",
    "    for k, i, j in new_cells\n",
    "]\n",
    "#############################################################\n",
    "\n",
    "#Add drains to the top of all the cells\n",
    "# Get indices where idomain == 1\n",
    "cells = np.argwhere(idomain == 1)\n",
    "\n",
    "# Convert to list of tuples\n",
    "cell_list = [tuple(cell) for cell in cells]\n",
    "\n",
    "river_cells = {(k, i, j) for k, i, j, *_ in river_drain}\n",
    "\n",
    "# Filter out river cells from cell_list\n",
    "filtered_cells = [cell for cell in cell_list if cell not in river_cells]\n",
    "\n",
    "#Filter out bear spring from the cell_list\n",
    "filtered_cells = [cell for cell in cell_list if cell not in BS_drain]\n",
    "\n",
    "top_drains = [(k, i, j, top[i, j], 1000) for (k, i, j) in filtered_cells]\n",
    "\n",
    "\n",
    "all_creek_drains = river_drain + new_river_drain_entries\n",
    "all_creek_drains = river_drain ##USE THIS WHEN WORKING WITH A LOWER GRID RESOLUTION\n",
    "drnspd = BS_drain + all_creek_drains + top_drains #combined list of the drain data to pass into MODFLOW DRN package ##I AM STILL HAVING ISSUES WITH THE river_drain!!\n",
    "\n",
    "#Drain package\n",
    "drn = flopy.mf6.ModflowGwfdrn(\n",
    "    gwf,\n",
    "    stress_period_data = drnspd,\n",
    "    save_flows = True,\n",
    "    print_flows = True\n",
    ")\n",
    "\n",
    "#initial conditions (estimated water table elevation)\n",
    "wtElev = obsGWE_df.iloc[0]['gw_elev[m]']\n",
    "strt = wtElev * np.ones((nlay ,nrow , ncol))\n",
    "strt[idomain == 0] = 0 #ensure the strt array is only assigned to active cells (idomain == 1))\n",
    "\n",
    "#Set the initial conditions for the model\n",
    "ic = flopy.mf6.ModflowGwfic(gwf, strt = strt)\n",
    "\n",
    "#node property flow package\n",
    "npf = flopy.mf6.ModflowGwfnpf(\n",
    "    gwf,\n",
    "    k = K11,\n",
    "    k33 = K33,\n",
    "    icelltype = 1,\n",
    "    save_specific_discharge = True\n",
    ")\n",
    "\n",
    "#define a specific yield value (using textbook example)\n",
    "sy = 0.2\n",
    "ss = 1E-5\n",
    "\n",
    "#Create the storage package (Relevant for the transient Model)\n",
    "sto = flopy.mf6.ModflowGwfsto(\n",
    "    gwf,\n",
    "    sy = sy,\n",
    "    ss_confined_only=False,\n",
    "    ss = ss\n",
    ")\n",
    "\n",
    "#set the output control module for ease of access later\n",
    "oc = flopy.mf6.ModflowGwfoc(gwf,\n",
    "                            budget_filerecord=f\"{name}.bud\",\n",
    "                            head_filerecord=f\"{name}.hds\",\n",
    "                            printrecord=[('HEAD', 'ALL'), ('BUDGET', 'ALL')],\n",
    "                            saverecord=[('HEAD', 'ALL'), ('BUDGET', 'ALL')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f25ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Plot the Kh and Kv distributions\n",
    "k11_view = npf.k.array \n",
    "k33_view = npf.k33.array\n",
    "\n",
    "#mask out the idomain inactive cells from the array\n",
    "k11_view = np.ma.masked_where(idomain == 0, k11_view)\n",
    "k33_view = np.ma.masked_where(idomain == 0, k33_view)\n",
    "\n",
    "#fig, axes = plt.subplots(2, nlay, figsize=(9, 9))\n",
    "fig, ax = plt.subplots(1, nlay, figsize=(12, 6.5))\n",
    "\n",
    "\n",
    "for i in range(nlay):\n",
    "    ax = ax\n",
    "    ax.set_title(f\"Layer {i + 1} Kh\")\n",
    "    ax.set_xlabel(\"Column\")\n",
    "    ax.set_ylabel(\"Row\")\n",
    "    im = ax.imshow(k11_view[i], cmap='viridis', origin='upper')\n",
    "    fig.colorbar(im, ax=ax, label = 'Kh (m/day)')\n",
    "    \n",
    "    #ax = axes[1]\n",
    "    #im = ax.imshow(k33_view[i], cmap='jet', origin='upper')\n",
    "    #ax.set_title(f\"Layer {i + 1} Kv\")\n",
    "    #ax.set_xlabel(\"Column\")\n",
    "    #ax.set_ylabel(\"Row\")\n",
    "    #fig.colorbar(im, ax=ax, label=\"Kv (m/day)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# write the MODFLOW input files\n",
    "sim.write_simulation(silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc18a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#run the MODFLOW simulation\n",
    "sim.run_simulation(silent= True) #Set silent to False if you want to print out the messages during the simulation run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8fff89",
   "metadata": {},
   "source": [
    "## Post Process the model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# load the output from the simulation\n",
    "head = gwf.output.head().get_alldata()\n",
    "bud = gwf.output.budget()\n",
    "spdis = bud.get_data(text='DATA-SPDIS')[0]\n",
    "qx, qy, qz = flopy.utils.postprocessing.get_specific_discharge(spdis, gwf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58149867",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Plot the first timestep to ensure that the model appears to be starting the sequence properly\n",
    "# create a plot of the model results\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 12))\n",
    "ax = axes[0] \n",
    "ax.set_title(\"Map View\")\n",
    "ax.set_aspect(1)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "pmv = flopy.plot.PlotMapView(gwf, ax=ax)\n",
    "max_head = np.max(head[head != 1e30]) #get the max head for the colorbar\n",
    "min_head = np.min(head)\n",
    "#pmv.plot_bc(ftype=\"CHD\") \n",
    "cb = pmv.plot_array(head[0][0]) #plot the head at the start of the model\n",
    "pmv.plot_inactive(color_noflow=\"gray\")\n",
    "pmv.plot_grid(color=\"black\", linewidth=0.1)\n",
    "#pmv.plot_vector(qx, qy, normalize=True, color=\"black\", istep=3, jstep=3)\n",
    "#pmv.contour_array(head[0][0], colors='k', linewidths = 0.75)\n",
    "\n",
    "plt.colorbar(cb, label = \"Head (m)\")\n",
    "\n",
    "# Get cell locations for Bear Creek\n",
    "river_cells = [(cell[1], cell[2]) for cell in all_creek_drains]  # Extract row and column indices from river_drain\n",
    "river_x = [sg.xcellcenters[row, col] for row, col in river_cells] # Get x coordinates of river cells in the grid\n",
    "river_y = [sg.ycellcenters[row, col] for row, col in river_cells] # Get y coordinates of river cells in the grid\n",
    "\n",
    "#Get the cell locations for Bear, Bear Overflow, and Hammel Springs\n",
    "spring_x = [sg.xcellcenters[row, col] for name, elev, row, col in spring_cells] # Get x coordinates of river cells in the grid\n",
    "spring_y = [sg.ycellcenters[row, col] for name, elev, row, col in spring_cells] # Get y coordinates of river cells in the grid\n",
    "\n",
    "#Get the cell locations for the Sinkholes in the model\n",
    "#Get the cell locations for Bear, Bear Overflow, and Hammel Springs\n",
    "sinkhole_x = [sg.xcellcenters[row, col] for name, elev, row, col in sinkhole_cells] # Get x coordinates of river cells in the grid\n",
    "sinkhole_y = [sg.ycellcenters[row, col] for name, elev, row, col in sinkhole_cells] # Get y coordinates of river cells in the grid\n",
    "\n",
    "#Get the cell locations for the MRS Well in the model: MRSW UTME: 557091, UTMN: 4867265\n",
    "welli, wellj = sg.intersect(557091, 4867265)\n",
    "\n",
    "#Get the water table elevation at the MRSW:\n",
    "MRS_head = head[0][0][welli][wellj]\n",
    "\n",
    "#Plot the creek, spring, and sinkhole cells on the map\n",
    "ax.scatter(river_x, river_y, color='blue', s=1, label = 'Bear Creek') # PLot the river_drain cells\n",
    "ax.scatter(spring_x, spring_y, color='magenta', s=5, label = 'Springs') # PLot the spring_drain cells\n",
    "ax.scatter(sinkhole_x, sinkhole_y, color = 'orange', s=5, label = 'Sinkholes')\n",
    "\n",
    "#Plot the location of the MRSW on the Plot\n",
    "ax.scatter(557091, 4867265, color='purple', label = 'MRS Well')\n",
    "\n",
    "ax.legend(loc = 'upper right')\n",
    "\n",
    "#PLot an array showing in which cells the head exceeds the land surface\n",
    "\n",
    "#Plot the locations where the hydraulic head exceeds the land surface elevation\n",
    "excess_head_mask = np.zeros_like(head[0][0])\n",
    "valid_mask = (head[0][0] < 1e30)  # Exclude cells with head >= 1e30\n",
    "excess_head_mask[(head[0][0] > top) & valid_mask] = 1  # Mark cells with valid excess head\n",
    "\n",
    "# Create an array to store the head difference\n",
    "head_difference = np.zeros_like(excess_head_mask)  # Initialize with zeros (or np.nan if preferred)\n",
    "\n",
    "# Record the head difference where the mask is active\n",
    "head_difference = head[0][0] - top\n",
    "\n",
    "head_difference[excess_head_mask == 0 ] = 0\n",
    "\n",
    "# Plot the model results in the second row\n",
    "ax = axes[1]\n",
    "ax.set_title(\"Map View\")\n",
    "ax.set_aspect(1)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "\n",
    "# Map view plot\n",
    "pmv = flopy.plot.PlotMapView(gwf, ax=ax)\n",
    "cb = pmv.plot_array(head[0][0])  # Plot the head at the start of the model\n",
    "pmv.plot_inactive(color_noflow=\"gray\")\n",
    "pmv.plot_grid(color=\"black\", linewidth=0.05)\n",
    "\n",
    "# Overlay the excess head mask\n",
    "#pmv.plot_array(excess_head_mask, cmap=\"Reds\", alpha=0.5, label=\"Excess Head\")\n",
    "cbx = pmv.plot_array(head_difference, cmap = \"Reds\", alpha = 0.4, label = \"Excess Head\")\n",
    "\n",
    "# Plot features\n",
    "ax.scatter(river_x, river_y, color='blue', s=1, label='Bear Creek')\n",
    "ax.scatter(spring_x, spring_y, color='magenta', s=1, label='Springs')\n",
    "ax.scatter(sinkhole_x, sinkhole_y, color='orange', s=1, label='Sinkholes')\n",
    "#ax.legend(loc='upper right')\n",
    "\n",
    "plt.colorbar(cb, label=\"Head (m)\")\n",
    "plt.colorbar(cbx, label = \"Excess Head (m)\")\n",
    "\n",
    "#Temporarily disable cross section plotting while using highly discretized grid\n",
    "#ax = axes[1]\n",
    "#ax.set_title(\"Cross Section\")\n",
    "#ax.set_aspect(10.)\n",
    "#ax.set_xlabel(\"x\")\n",
    "#ax.set_ylabel(\"z\")\n",
    "#pxs = flopy.plot.PlotCrossSection(gwf, ax=ax, line={\"row\": int(nrow*0.75)})\n",
    "#pxs.plot_inactive(color_noflow=\"gray\")\n",
    "#pxs.plot_bc(ftype=\"CHD\")\n",
    "#pxs.plot_array(head, head=head)\n",
    "#cb = pxs.plot_array(head, head=head)\n",
    "#pxs.plot_grid(color=\"black\", linewidth=0.05)\n",
    "#plt.colorbar(cb, orientation='horizontal', label = 'Head (m)')\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "welli, wellj = sg.intersect(557091, 4867265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6806e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save the images of head and land surface exceedence in a folder for making GIFs with\n",
    "output_folder = pl.Path(ws) / 'head_plots'\n",
    "#output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Clear the folder if it exists\n",
    "if output_folder.exists():\n",
    "    for item in output_folder.iterdir():\n",
    "        if item.is_file():\n",
    "            item.unlink()  # Remove file\n",
    "        elif item.is_dir():\n",
    "            shutil.rmtree(item)  # Remove directory and its contents\n",
    "\n",
    "# Ensure the folder is recreated\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "#Reload the Head Data\n",
    "head = gwf.output.head().get_alldata()\n",
    "\n",
    "fig_interval = 12 #Number of hours between successive figures\n",
    "\n",
    "startDate = datetime(2020, 5, 1)\n",
    "\n",
    "#Get the global min and max head so that the colorbar remains static throuhgout the duration of the experiments\n",
    "max_head = np.nanmax(head[head != 1e30]) #get the max head for the colorbar\n",
    "min_head = np.nanmin(head)\n",
    "\n",
    "galena_Head = []\n",
    "for index, head in enumerate(head):\n",
    "    #Get the head at every timestep (Not just at the start of each SPD)\n",
    "    #Get the water table elevation at the MRSW:\n",
    "    MRS_head = head[0][welli][wellj]\n",
    "    \n",
    "    #Record the head at the MRSW in the list to be plotted later\n",
    "    galena_Head.append(MRS_head)\n",
    "    \n",
    "    if index % fig_interval != 0:  # Skip indices that are not multiples of fig_interval\n",
    "        continue\n",
    "    #Create the plots of the head distribution and the error cells over the entire time of the model\n",
    "    # create a plot of the model results\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Compute the current datetime for the figure\n",
    "    elapsed_hours = index # Convert index to hours based on fig_interval\n",
    "    current_time = startDate + timedelta(hours=elapsed_hours)  # Adjust from start date\n",
    "    # Add title with formatted date and time\n",
    "    fig.suptitle(f\"Head Distribution and Exceedance - {current_time.strftime('%Y-%m-%d %H:%M')}\", \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "    \n",
    "    ax = axes[0] \n",
    "    ax.set_title(\"Map View: Head Distribution\")\n",
    "    ax.set_aspect(1)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    pmv = flopy.plot.PlotMapView(gwf, ax=ax)\n",
    "    \n",
    "    #pmv.plot_bc(ftype=\"CHD\") \n",
    "    cb = pmv.plot_array(head, vmin = min_head, vmax = max_head) #plot the head at the start of the model\n",
    "    pmv.plot_inactive(color_noflow=\"gray\")\n",
    "    #pmv.plot_grid(color=\"black\", linewidth=0.1)\n",
    "    spdis = bud.get_data(text='DATA-SPDIS')[index]\n",
    "    qx, qy, qz = flopy.utils.postprocessing.get_specific_discharge(spdis, gwf)\n",
    "    pmv.plot_vector(qx, qy, normalize=True, color=\"black\", istep=3, jstep=3)\n",
    "    pmv.contour_array(head, colors='k', linewidths = 0.75)\n",
    "\n",
    "    plt.colorbar(cb, label = \"Head (m)\")\n",
    "\n",
    "    # Get cell locations for Bear Creek\n",
    "    river_cells = [(cell[1], cell[2]) for cell in all_creek_drains]  # Extract row and column indices from river_drain\n",
    "    river_x = [sg.xcellcenters[row, col] for row, col in river_cells] # Get x coordinates of river cells in the grid\n",
    "    river_y = [sg.ycellcenters[row, col] for row, col in river_cells] # Get y coordinates of river cells in the grid\n",
    "\n",
    "    #Get the cell locations for Bear, Bear Overflow, and Hammel Springs\n",
    "    spring_x = [sg.xcellcenters[row, col] for name, elev, row, col in spring_cells] # Get x coordinates of river cells in the grid\n",
    "    spring_y = [sg.ycellcenters[row, col] for name, elev, row, col in spring_cells] # Get y coordinates of river cells in the grid\n",
    "\n",
    "    #Get the cell locations for the Sinkholes in the model\n",
    "    #Get the cell locations for Bear, Bear Overflow, and Hammel Springs\n",
    "    sinkhole_x = [sg.xcellcenters[row, col] for name, elev, row, col in sinkhole_cells] # Get x coordinates of river cells in the grid\n",
    "    sinkhole_y = [sg.ycellcenters[row, col] for name, elev, row, col in sinkhole_cells] # Get y coordinates of river cells in the grid\n",
    "\n",
    "    #Get the cell locations for the MRS Well in the model: MRSW UTME: 557091, UTMN: 4867265\n",
    "    welli, wellj = sg.intersect(557091, 4867265)\n",
    "\n",
    "    \n",
    "    #Plot the creek, spring, and sinkhole cells on the map\n",
    "    ax.scatter(river_x, river_y, color='blue', s=1, label = 'Bear Creek') # PLot the river_drain cells\n",
    "    ax.scatter(spring_x, spring_y, color='magenta', s=5, label = 'Springs') # PLot the spring_drain cells\n",
    "    ax.scatter(sinkhole_x, sinkhole_y, color = 'orange', s=5, label = 'Sinkholes')\n",
    "\n",
    "    #Plot the location of the MRSW on the Plot\n",
    "    ax.scatter(557091, 4867265, color='purple', label = 'MRS Well')\n",
    "\n",
    "    ax.legend(loc = 'upper right')\n",
    "\n",
    "    #PLot an array showing in which cells the head exceeds the land surface\n",
    "\n",
    "    #Plot the locations where the hydraulic head exceeds the land surface elevation\n",
    "    excess_head_mask = np.zeros_like(head[0])\n",
    "    valid_mask = (head[0] < 1e30)  # Exclude cells with head >= 1e30\n",
    "    excess_head_mask[(head[0] > top) & valid_mask] = 1  # Mark cells with valid excess head\n",
    "\n",
    "    # Create an array to store the head difference\n",
    "    head_difference = np.zeros_like(excess_head_mask)  # Initialize with zeros (or np.nan if preferred)\n",
    "\n",
    "    # Record the head difference where the mask is active\n",
    "    head_difference = head[0] - top\n",
    "\n",
    "    head_difference[excess_head_mask == 0 ] = 0\n",
    "\n",
    "    # Plot the model results in the second row\n",
    "    ax = axes[1]\n",
    "    ax.set_title(\"Map View: Head Over Land Surface\")\n",
    "    ax.set_aspect(1)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "    # Map view plot\n",
    "    pmv = flopy.plot.PlotMapView(gwf, ax=ax)\n",
    "    cb = pmv.plot_array(head, vmin = min_head, vmax = max_head)  # Plot the head at the start of the model\n",
    "    pmv.plot_inactive(color_noflow=\"gray\")\n",
    "    pmv.plot_grid(color=\"black\", linewidth=0.05)\n",
    "\n",
    "    # Overlay the excess head mask\n",
    "    #pmv.plot_array(excess_head_mask, cmap=\"Reds\", alpha=0.5, label=\"Excess Head\")\n",
    "    cbx = pmv.plot_array(head_difference, cmap = \"Reds\", alpha = 0.4, label = \"Excess Head\")\n",
    "\n",
    "    # Plot features\n",
    "    ax.scatter(river_x, river_y, color='blue', s=1, label='Bear Creek')\n",
    "    ax.scatter(spring_x, spring_y, color='magenta', s=1, label='Springs')\n",
    "    ax.scatter(sinkhole_x, sinkhole_y, color='orange', s=1, label='Sinkholes')\n",
    "    #ax.legend(loc='upper right')\n",
    "\n",
    "    plt.colorbar(cb, label=\"Head (m)\")\n",
    "    plt.colorbar(cbx, label = \"Excess Head (m)\")\n",
    "    \n",
    "    \n",
    "    fig.savefig(pl.Path(output_folder, f\"spd_{int(index/fig_interval)}.png\"))\n",
    "    plt.close(fig)\n",
    "\n",
    "# Set the output file path for the GIF\n",
    "gif_path = pl.Path(output_folder) / \"head_evolution.gif\"\n",
    "\n",
    "# Get a sorted list of all images\n",
    "image_files = sorted(output_folder.glob(\"spd_*.png\"), key=lambda x: int(x.stem.split(\"_\")[1]))\n",
    "\n",
    "# Read images and save as GIF\n",
    "images = [imageio.imread(img) for img in image_files]\n",
    "imageio.mimsave(gif_path, images, format= 'GIF' , duration=0.5)  # Adjust duration for speed control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc33665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the discharge from Bear Spring\n",
    "\n",
    "#Get the Bear Spring Coordinates\n",
    "print(f'The Coordinates for Bear Spring are:{kBS, iBS, jBS}')\n",
    "\n",
    "#get the discharge from the BS drain and River drains\n",
    "cbb_file = f\"{ws}/{name}.bud\"\n",
    "cbb = flopy.utils.CellBudgetFile(cbb_file, precision = 'double')\n",
    "\n",
    "#Get all of the drain data from the cbb\n",
    "drn_discharge = cbb.get_data(text='DRN') #retrieve all of the DRN flows\n",
    "\n",
    "disch_array = np.array(drn_discharge) #convert the nested array to a format where it can actually be read\n",
    "\n",
    "#calculate the total discharge through bear spring over the entire model, units of L**3\n",
    "BS_m3 = 0\n",
    "for i in range(len(disch_array[:,0])): #Bear Spring is the 0th index in the drn_array\n",
    "    BS_m3 = BS_m3 + disch_array[:,0][i][2] * -1\n",
    "BS_m3 = BS_m3 / 24 # ONLY RELEVANT FOR TRANSIENT MODEL #convert to the correct units to account for the hourly tsteps (raw values given in m3/day, but the # of tsteps is less)\n",
    "print(f\"Total Bear Spring Discharge: {BS_m3:.0f} m\")\n",
    "\n",
    "#Bear Creek Drain Cells total discharge\n",
    "BC_m3 = 0 #Initialize the total discharge sum\n",
    "\n",
    "# Loop over all timesteps (720 timesteps)\n",
    "for i in range(np.shape(disch_array)[0]):  # Iterate over   timesteps (rows of drn_array)\n",
    "    # Loop over all drains, skipping the 0th drain\n",
    "    for n in range(1, np.shape(disch_array)[1]):  # Start at n=1 to skip the 0th drain\n",
    "        # Access the discharge value (3rd element of the tuple)\n",
    "        BC_m3 += abs(disch_array[i, n][2])  # Add the absolute discharge value\n",
    "        \n",
    "BC_m3 = BC_m3 / 4 #ONLY RELEVANT FOR TRANSIENT MODEL #convert to the correct units to account for the hourly tsteps (raw values given in m3/day, but the # of tsteps is less)   \n",
    "\n",
    "# Print the total discharge in cubic meters (m)\n",
    "print(f\"Total Bear Creek Discharge: {BC_m3:.0f} m\")\n",
    "total_drain = BC_m3 + BS_m3\n",
    "print(f\"Total System Discharge: {total_drain:.0f} m\")\n",
    "\n",
    "#Get the head value in the cell that is asssociated with the Galena MRS well to use as an additional calibration check point\n",
    "print(f'The Water Table Elevation in the MRS well is: {MRS_head} m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd79c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the discharge timeseries produced by the model\n",
    "#Get the actual observed timeseries\n",
    "obsDis_df['Q[m3/day]'] = obsDis_df['Discharge[m3/s]'] * 86400 #create a column using m3/day units\n",
    "\n",
    "tstep = nstp #Hours\n",
    "\n",
    "#Resample the dataframe to match the tstep intervals created by the MF model\n",
    "print(f'The number of steps in each stress period is: {nstp}, which translates to {tstep} hours per timestep')\n",
    "dis_avg = obsDis_df.resample(f'{tstep}h').mean() #Average the daily values\n",
    "\n",
    "#precipitation = precip_df.resample(f'{tstep}h').mean() * -1 #Negative to make them plot from top down\n",
    "\n",
    "bearDis = disch_array[:, 0]['q'] * -1\n",
    "\n",
    "creekDis = disch_array[:, 1:]['q'].sum(axis=1) * -1\n",
    "\n",
    "datetimes = pd.date_range(start='05/01/2019 00:00', end=\"08/31/2019 23:45\", freq=f\"{tstep}h\").to_list()\n",
    "\n",
    "#precipitation.iloc[np.argmin(precipitation['PRCP'])]\n",
    "dis_avg.iloc[np.argmax(dis_avg['Q[m3/day]'])]\n",
    "\n",
    "\n",
    "print('The Elevation of the MRSW is 382.5 m above NAVD88')\n",
    "fig, axes = plt.subplots(ncols=1, nrows=2, figsize=(12,9))\n",
    "ax = axes[0]\n",
    "ax.scatter(datetimes, bearDis, label = 'Modeled Discharge', s = 5)\n",
    "ax.set_title('Bear Spring Discharge', fontsize = 16)\n",
    "ax.set_xlabel('Datetime', fontsize = 14)\n",
    "ax.set_ylabel('Discharge, m^3/day', fontsize = 14)\n",
    "ax.scatter(datetimes, dis_avg['Q[m3/day]'], label = 'Observed_Discharge', s = 5)\n",
    "ax.tick_params(labelsize = 12)\n",
    "ax.set_ylim(0, 100000)\n",
    "#ax.set_ylim([np.min(dis_avg['Q[m3/day]']) - 10000.0, np.max(dis_avg['Q[m3/day]']) + 10000.0])\n",
    "\n",
    "# Plot precipitation as downward bars\n",
    "# Plot Recharge as downward bars\n",
    "ax2 = ax.twinx()\n",
    "bar_width = pd.Timedelta(f\"{tstep}h\") / pd.Timedelta(\"1D\")  # Normalize width to a fraction of a day\n",
    "ax2.bar(datetimes, rch_df['RCH[m/day]'], width=bar_width, color='b', alpha=0.5, label='Recharge')\n",
    "\n",
    "#ax2.scatter(datetimes, rch_df['RCH[m/day]'] , color='r', alpha = 0.25, label = 'Recharge [m/day]', s = 5) #width=0.1, align='center', label = 'precipitation')\n",
    "ax.legend(loc = 'center', fontsize = 14)\n",
    "ax2.legend(loc='center right', fontsize = 14)\n",
    "ax2.set_ylim(0, 0.03)\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_ylabel('RCH [m/day]', fontsize  = 14)\n",
    "ax2.tick_params(labelsize = 12)\n",
    "#ax = axes[1]\n",
    "#ax.scatter(datetimes, creekDis)\n",
    "#ax.set_title('Bear Creek Discharge')\n",
    "#ax.set_xlabel('Datetime')\n",
    "#ax.set_ylabel('Discharge, m^3/day')\n",
    "\n",
    "ax= axes[1]\n",
    "ax.set_title('MRSW Head Elevation', fontsize = 16)\n",
    "ax.scatter(obsGWE_df.index, obsGWE_df['gw_elev[m]'], label = 'observed MRSW head',  s = 5)\n",
    "ax.scatter(datetimes, galena_Head, color = 'green', label=' Modeled MRSW Head',  s = 5)\n",
    "ax.set_ylabel('GW Elevation above NAVD88, m', fontsize = 14)\n",
    "ax.legend(loc='best', fontsize = 14)\n",
    "ax.tick_params(labelsize = 12)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c984b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the discharge timeseries produced by the model\n",
    "#Get the actual observed timeseries\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "obsDis_df['Q[m3/day]'] = obsDis_df['Discharge[m3/s]'] * 86400 #create a column using m3/day units\n",
    "\n",
    "tstep = nstp #Hours\n",
    "\n",
    "\n",
    "#Resample the dataframe to match the tstep intervals created by the MF model\n",
    "print(f'The number of steps in each stress period is: {nstp}, which translates to {tstep} hours per timestep')\n",
    "dis_avg = obsDis_df.resample(f'{tstep}h').mean() #Average the daily values\n",
    "\n",
    "#precipitation = precip_df.resample(f'{tstep}h').mean() * -1 #Negative to make them plot from top down\n",
    "\n",
    "bearDis = disch_array[:, 0]['q'] * -1\n",
    "\n",
    "creekDis = disch_array[:, 1:]['q'].sum(axis=1) * -1\n",
    "\n",
    "datetimes = pd.date_range(start='05/01/2019 00:00', end=\"08/31/2019 23:45\", freq=f\"{tstep}h\").to_list()\n",
    "\n",
    "#precipitation.iloc[np.argmin(precipitation['PRCP'])]\n",
    "dis_avg.iloc[np.argmax(dis_avg['Q[m3/day]'])]\n",
    "\n",
    "\n",
    "print('The Elevation of the MRSW is 382.5 m above NAVD88')\n",
    "fig, axes = plt.subplots(ncols=1, nrows=2, figsize=(12,9))\n",
    "fig.suptitle('2019 LCA Transient Results', fontsize=20, fontweight='bold')\n",
    "\n",
    "ax = axes[0]\n",
    "ax.scatter(datetimes, bearDis, label = 'Modeled Discharge', s = 5, marker = '^')\n",
    "ax.set_title('Bear Spring Discharge', fontsize = 18)\n",
    "ax.set_xlabel('Date', fontsize = 16)\n",
    "ax.set_ylabel(r'Discharge [m$^3$/day]', fontsize = 16)\n",
    "ax.scatter(datetimes, dis_avg['Q[m3/day]'], label = 'Observed Discharge', s = 5)\n",
    "ax.tick_params(labelsize = 14)\n",
    "ax.set_ylim(0, 100000)\n",
    "\n",
    "#ax.set_ylim([np.min(dis_avg['Q[m3/day]']) - 10000.0, np.max(dis_avg['Q[m3/day]']) + 10000.0])\n",
    "\n",
    "# Plot precipitation as downward bars\n",
    "# Plot Recharge as downward bars\n",
    "ax2 = ax.twinx()\n",
    "bar_width = pd.Timedelta(f\"{tstep}h\") / pd.Timedelta(\"1D\")  # Normalize width to a fraction of a day\n",
    "ax2.bar(datetimes, rch_df['RCH[m/day]'], width=bar_width, color='dodgerblue', alpha=0.3, label='Recharge')\n",
    "\n",
    "#ax2.scatter(datetimes, rch_df['RCH[m/day]'] , color='r', alpha = 0.25, label = 'Recharge [m/day]', s = 5) #width=0.1, align='center', label = 'precipitation')\n",
    "#ax.legend(loc = 'center', fontsize = 14)\n",
    "#ax2.legend(loc='center right', fontsize = 14)\n",
    "ax2.set_ylim(0, 0.02)\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_ylabel('Recharge [m/day]', fontsize  = 16)\n",
    "ax2.tick_params(labelsize = 14)\n",
    "#ax = axes[1]\n",
    "#ax.scatter(datetimes, creekDis)\n",
    "#ax.set_title('Bear Creek Discharge')\n",
    "#ax.set_xlabel('Datetime')\n",
    "#ax.set_ylabel('Discharge, m^3/day')\n",
    "\n",
    "# Get handles and labels from both axes\n",
    "handles1, labels1 = ax.get_legend_handles_labels()   # From primary y-axis\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()  # From secondary y-axis\n",
    "\n",
    "# Combine both legend entries\n",
    "#ax.legend(handles1 + handles2, labels1 + labels2, loc=\"upper right\", fontsize=16, markerscale = 4,\n",
    "#             bbox_to_anchor=(1.01, 0.85)  # Adjust vertical position (lower values move it down)\n",
    "#)\n",
    "\n",
    "# Set major ticks to every month (adjust as needed)\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "\n",
    "# Optionally, format the labels to make them more readable\n",
    "#ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))  # Example: \"May 2019\"\n",
    "\n",
    "# Rotate labels for better visibility\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "ax= axes[1]\n",
    "ax.set_title('MRSW Head Elevation', fontsize = 16)\n",
    "ax.scatter(datetimes, galena_Head, label='Modeled MRSW Head',  s = 5, marker = '^')\n",
    "ax.scatter(obsGWE_df.index, obsGWE_df['gw_elev[m]'], color = 'green', label = 'Observed MRSW head',  s = 5)\n",
    "ax.set_ylabel('GW Elevation [m]', fontsize = 16)\n",
    "#ax.legend(loc='lower right', fontsize = 16, markerscale = 4)\n",
    "ax.tick_params(labelsize = 14)\n",
    "ax.set_xlabel('Date', fontsize = 16)\n",
    "\n",
    "# Set major ticks to every month (adjust as needed)\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "\n",
    "# Optionally, format the labels to make them more readable\n",
    "#ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))  # Example: \"May 2019\"\n",
    "\n",
    "# Rotate labels for better visibility\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
